# -*- coding: utf-8 -*-
"""autolysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IvmWwXQJZ1PqeOaAopDpCnNSkjBmofw9
"""

pip install openai==0.27.8

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import argparse
import openai  # LLM interaction library

# Function to get API token
def get_api_token():
    token = 'eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjI0ZjEwMDE0NTBAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.uwUSg9eNHbiYGaJmcsADqeWZCYAhuyp4cnPrCNWEy3Q'
    if not token:
        raise ValueError("API Token is required.")
    return token

# Analyze the data
def analyze_data(df):
    summary_stats = df.describe()
    missing_values = df.isnull().sum()
    numeric_df = df.select_dtypes(include=[np.number])
    corr_matrix = numeric_df.corr() if not numeric_df.empty else pd.DataFrame()
    return summary_stats, missing_values, corr_matrix

# Detect outliers
def detect_outliers(df):
    df_numeric = df.select_dtypes(include=[np.number])
    Q1 = df_numeric.quantile(0.25)
    Q3 = df_numeric.quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).sum()
    return outliers

# Visualize the data
def visualize_data(corr_matrix, outliers, output_dir):
    heatmap_file = os.path.join(output_dir, 'correlation_matrix.png')
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title('Correlation Matrix')
    plt.savefig(heatmap_file)
    plt.close()

    if not outliers.empty and outliers.sum() > 0:
        outliers_file = os.path.join(output_dir, 'outliers.png')
        plt.figure(figsize=(10, 6))
        outliers.plot(kind='bar', color='red')
        plt.title('Outliers Detection')
        plt.xlabel('Columns')
        plt.ylabel('Number of Outliers')
        plt.savefig(outliers_file)
        plt.close()
    else:
        print("No outliers detected to visualize.")
        outliers_file = None

    return heatmap_file, outliers_file

# Create README.md
def create_readme(summary_stats, missing_values, corr_matrix, outliers, output_dir, story):
    readme_file = os.path.join(output_dir, 'README.md')
    with open(readme_file, 'w') as f:
        f.write("# Automated Data Analysis Report\n\n")
        f.write("## Summary Statistics\n")
        f.write(f"{summary_stats}\n\n")

        f.write("## Missing Values\n")
        f.write(f"{missing_values}\n\n")

        f.write("## Outliers Detection\n")
        f.write(f"{outliers}\n\n")

        f.write("## Correlation Matrix\n")
        f.write("Below is the correlation matrix of numerical features:\n\n")
        f.write("![Correlation Matrix](correlation_matrix.png)\n\n")

        f.write("## Outliers Visualization\n")
        if outliers is not None:
            f.write("Below is the outliers detection chart:\n\n")
            f.write("![Outliers](outliers.png)\n")
        else:
            f.write("No significant outliers detected to visualize.\n\n")

        f.write("## Data Story\n")
        f.write(f"{story}\n")
    return readme_file

# Generate a story using the LLM
def generate_story(summary_stats, missing_values, corr_matrix, outliers):
    openai.api_key = os.environ.get("AIPROXY_TOKEN")
    if not openai.api_key:
        raise ValueError("AIPROXY_TOKEN is not set in the environment variables.")

    prompt = (
        f"Write a compelling story summarizing the data analysis results:\n"
        f"1. Summary statistics:\n{summary_stats}\n"
        f"2. Missing values:\n{missing_values}\n"
        f"3. Correlation insights:\n{corr_matrix}\n"
        f"4. Outliers detected:\n{outliers}\n\n"
        f"Describe the data characteristics, key findings, and their implications."
    )
    try:
        # Adjust the engine to GPT-4o-Mini (via AI Proxy)
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant skilled at narrating data insights."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
            max_tokens=10,
        )
        story = response["choices"][0]["message"]["content"]
    except openai.error.OpenAIError as e:
        print(f"Error generating story: {e}")
        story = "An error occurred while generating the story."
    return story


# Main function
def main(csv_file):
    api_token = get_api_token()
    os.environ["AIPROXY_TOKEN"] = api_token

    try:
        df = pd.read_csv(csv_file, encoding='ISO-8859-1')
    except UnicodeDecodeError as e:
        print(f"Error reading file: {e}")
        return

    summary_stats, missing_values, corr_matrix = analyze_data(df)
    outliers = detect_outliers(df)
    output_dir = os.path.splitext(csv_file)[0]
    os.makedirs(output_dir, exist_ok=True)

    heatmap_file, outliers_file = visualize_data(corr_matrix, outliers, output_dir)
    story = generate_story(summary_stats, missing_values, corr_matrix, outliers)
    readme_file = create_readme(summary_stats, missing_values, corr_matrix, outliers, output_dir, story)

    print(f"Analysis complete. Results saved in '{output_dir}' directory.")
    print(f"README file: {readme_file}")
    print(f"Visualizations: {heatmap_file}, {outliers_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Automated Data Analysis")
    parser.add_argument("csv_file", help="CSV file for analysis")
    args = parser.parse_args(['media.csv'])
    main(args.csv_file)

